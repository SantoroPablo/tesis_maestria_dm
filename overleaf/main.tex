\documentclass[spanish]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{indentfirst}
\usepackage[a4paper, margin=1 in]{geometry}
\usepackage[spanish]{babel}

% Paquetes para usar graficos desde mathcha.io
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{tikz}
\usepackage{mathdots}
\usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\graphicspath{ {./images/} }
\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        

\begin{document}

\input{plan_tesis_title_page.tex}
\input{abstract_es.tex}
\input{abstract_en.tex}

\section{Introducción}

Dentro de la ciencia de datos actual, una de las áreas que más se
están desarrollando en los últimos tiempos es la relacionada a las
arquitecturas de redes neuronales y, en particular, a aquellas relacionadas
con redes profundas, o ``Deep Learning'' en inglés.

Teóricamente, cualquier red neuronal de tres capas (entrada, oculta
y de salida) puede aproximar cualquier función, pero aún se desconoce
el entrenamiento apropiado para poder alcanzar esa meta teórica.
Por ende, se recurrió a agregar capas no redundantes que permiten
alcanzar mejores resultados a los que se estaban logrando a través
de las redes neuronales de tres capas, dando lugar a las llamadas
redes profundas.

Las redes profundas incidieron de manera decisiva en el procesamiento
de imágenes en dos y tres dimensiones. Se han desarrollado y se continúan
desarrollando redes con cada vez mejor precisión a la hora de reconocer
y reconstruir figuras de las más diversas. Para la reconstrucción
de modelos tridimensionales, las arquitecturas más comunes son los ``autoencoders'',
las llamadas ``Generative Adversarial Networks'' (GAN) y los ``variational
autoencoders'' (VAE). Recientemente se han comenzaron a desarrollar
arquitecturas que, además de poder reconstruir los ejemplos de entrenamiento,
puede generar nuevas imágenes, tomando nuevas muestras de datos de
un espacio continuo multivariado \cite{Karras2018}.

Un campo que está comenzando a tomar impulso recientemente es el de
procesamiento de datos en tres dimensiones usando redes generativas
con arquitecturas como GAN y VAE. Estas redes permiten, además de
reconstruir figuras con las que se entrenen, ajustar una distribución
multivariada (ajustada a tener una distribución normal estándar multivariada)
a los fines de tener un espacio de muestreo continuo a través del cual poder generar
nuevas figuras y poder hacer una interpolación continua en el espacio continuo de
las figuras de entrenamiento. Esto permite poder explorar el espacio completo de las
figuras de entrenamiento, con la posibilidad de encontrar figuras relacionadas a las
originales pero con variaciones en los detalles, los cuales podrían ser útiles a los
fines de entender y explorar el espacio de figuras con las que se trabajó en los
datos originales.

\subsection{Convoluciones, pooling, normalización por lotes}

% En la versión de 2017 del libro "Deep Learning" de Goodfellow, Bengio y Courville
% a partir de la página 330 comienza el capítulo de redes convolucionales.
Las redes convolucionales son redes neuronales en las que al menos una de sus capas
usan una operación de convolución en lugar de una multiplicación matricial
\cite{goodfellow2016deep}. Este tipo de capas es fundamental para el procesamiento
de tipos de datos que presentan un formato de cuadrícula, como las imágenes
bidimensionales o los objetos tridimensionales. Es por ello que son uno de los
bloques fundamentales para las redes que tratan con este tipo de datos.

La convolución es una operación matemática que consiste en una media ponderada sobre
los datos de entrada (en el caso de \(t\) discreto):

\[s(t) = (x \circledast w)(t) = \sum_{a = -\infty}^{\infty} x(a)w(t-a) \]

Donde \(x\) representa los datos de entrada, \(w\) son los pesos del promedio
ponderado, que también se denominan \textit{kernel}.
Para el caso discreto bidimensional discreto (imágenes, por ejemplo), en lugar de
una suma infinita sólo se realiza sobre la suma de los valores de las matrices de
entrada a la convolución. También el kernel \(w\) toma la forma de una matriz
bidimensional con pesos adaptables por la red. Por ende, en el caso de una imagen
\(I\) con un kernel \(K\):

\[S(i,j)=(I \circledast K)(i,j)=\sum_{m}^{n} I(m,n)K(i-m,j-n)\]

Por ejemplo, al aplicar la convolución a una matriz de entrada bidimensional:

\begin{figure}[H]
\centering
\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,460); %set diagram left start at 0, and has height of 460

\draw   (115,33) -- (165,33) -- (165,83) -- (115,83) -- cycle ; %Shape: Square [id:dp008348541907688967] 
\draw   (177,33) -- (227,33) -- (227,83) -- (177,83) -- cycle ; %Shape: Square [id:dp5695495374345101] 
\draw   (238,33) -- (288,33) -- (288,83) -- (238,83) -- cycle ; %Shape: Square [id:dp08913217477595681] 
\draw   (300,33) -- (350,33) -- (350,83) -- (300,83) -- cycle ; %Shape: Square [id:dp09565678498451557] 
\draw   (115,93) -- (165,93) -- (165,143) -- (115,143) -- cycle ; %Shape: Square [id:dp6616061503281512] 
\draw   (177,93) -- (227,93) -- (227,143) -- (177,143) -- cycle ; %Shape: Square [id:dp9731322863310219] 
\draw   (238,93) -- (288,93) -- (288,143) -- (238,143) -- cycle ; %Shape: Square [id:dp1734488588540104] 
\draw   (300,93) -- (350,93) -- (350,143) -- (300,143) -- cycle ; %Shape: Square [id:dp783784734771654] 
\draw   (115,153) -- (165,153) -- (165,203) -- (115,203) -- cycle ; %Shape: Square [id:dp3810445646111744] 
\draw   (177,153) -- (227,153) -- (227,203) -- (177,203) -- cycle ; %Shape: Square [id:dp597477459287147] 
\draw   (238,153) -- (288,153) -- (288,203) -- (238,203) -- cycle ; %Shape: Square [id:dp3259061174876716] 
\draw   (300,153) -- (350,153) -- (350,203) -- (300,203) -- cycle ; %Shape: Square [id:dp9728278094881708] 
\draw   (112,29) -- (230,29) -- (230,147) -- (112,147) -- cycle ; %Shape: Square [id:dp8155679517421655] 
\draw   (399,60) -- (449,60) -- (449,110) -- (399,110) -- cycle ; %Shape: Square [id:dp394644813826424] 
\draw   (461,60) -- (511,60) -- (511,110) -- (461,110) -- cycle ; %Shape: Square [id:dp8614370121086297] 
\draw   (399,120) -- (449,120) -- (449,170) -- (399,170) -- cycle ; %Shape: Square [id:dp595415369573254] 
\draw   (461,120) -- (511,120) -- (511,170) -- (461,170) -- cycle ; %Shape: Square [id:dp8309992031835938] 
\draw   (396,56) -- (514,56) -- (514,174) -- (396,174) -- cycle ; %Shape: Square [id:dp77375541391222] 
\draw   (117,233) -- (213.5,233) -- (213.5,329.5) -- (117,329.5) -- cycle ; %Shape: Square [id:dp2906572996014607] 
\draw   (111,228) -- (219.5,228) -- (219.5,336.5) -- (111,336.5) -- cycle ; %Shape: Square [id:dp4766592365283635] 
\draw   (224,234) -- (320.5,234) -- (320.5,330.5) -- (224,330.5) -- cycle ; %Shape: Square [id:dp690329106444943] 
\draw   (330,234) -- (426.5,234) -- (426.5,330.5) -- (330,330.5) -- cycle ; %Shape: Square [id:dp14628491601258453] 
\draw   (117,344) -- (213.5,344) -- (213.5,440.5) -- (117,440.5) -- cycle ; %Shape: Square [id:dp268759623182556] 
\draw   (224,345) -- (320.5,345) -- (320.5,441.5) -- (224,441.5) -- cycle ; %Shape: Square [id:dp5482508582899983] 
\draw   (330,345) -- (426.5,345) -- (426.5,441.5) -- (330,441.5) -- cycle ; %Shape: Right Angle [id:dp2438723945511756] 
\draw   (62.09,279.98) -- (62.09,86.11) -- (111.33,86.11) ; %Straight Lines [id:da4032615230182004] 
\draw   (62.09,279.98) -- (109.33,279.98) ;
\draw [shift={(111.33,279.98)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0}][line width=0.75] (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29);


\draw (140,58) node   [align=left] {$\displaystyle a$}; % Text Node
\draw (202,58) node   [align=left] {$\displaystyle b$}; % Text Node
\draw (264,58) node   [align=left] {$\displaystyle c$}; % Text Node
\draw (326,58) node   [align=left] {$\displaystyle d$}; % Text Node
\draw (140,118) node   [align=left] {$\displaystyle e$}; % Text Node
\draw (202,118) node   [align=left] {$\displaystyle f$}; % Text Node
\draw (264,118) node   [align=left] {$\displaystyle g$}; % Text Node
\draw (326,118) node   [align=left] {$\displaystyle h$}; % Text Node
\draw (140,178) node   [align=left] {$\displaystyle i$}; % Text Node
\draw (202,178) node   [align=left] {$\displaystyle j$}; % Text Node
\draw (264,178) node   [align=left] {$\displaystyle k$}; % Text Node
\draw (326,178) node   [align=left] {$\displaystyle l$}; % Text Node
\draw (129,16) node   [align=left] {Input}; % Text Node
\draw (424,85) node   [align=left] {$\displaystyle w$}; % Text Node
\draw (486,85) node   [align=left] {$\displaystyle x$}; % Text Node
\draw (424,145) node   [align=left] {$\displaystyle y$}; % Text Node
\draw (486,145) node   [align=left] {$\displaystyle z$}; % Text Node
\draw (418,43) node   [align=left] {Kernel}; % Text Node
% Text Node
\draw (169,284) node   [align=left] {$\displaystyle
\begin{array}{{>{\displaystyle}l}}
aw\ +bx\ +\ \\
ey\ \ +\ fz
\end{array}$};
% Text Node
\draw (276,285) node   [align=left] {$\displaystyle
\begin{array}{{>{\displaystyle}l}}
bw\ +cx\ +\ \\
fy\ \ +\ gz
\end{array}$};
% Text Node
\draw (382,286) node   [align=left] {$\displaystyle
\begin{array}{{>{\displaystyle}l}}
cw\ +dx\ +\ \\
gy\ \ +\ hz
\end{array}$};
% Text Node
\draw (168,393) node   [align=left] {$\displaystyle
\begin{array}{{>{\displaystyle}l}}
ew\ +fx\ +\ \\
iy\ \ +\ jz
\end{array}$};
% Text Node
\draw (275,394) node   [align=left] {$\displaystyle
\begin{array}{{>{\displaystyle}l}}
fw\ +gx\ +\ \\
jy\ \ +\ kz
\end{array}$};
% Text Node
\draw (381,395) node   [align=left] {$\displaystyle
\begin{array}{{>{\displaystyle}l}}
gw\ +hx\ +\ \\
ky\ \ +\ lz
\end{array}$};

\end{tikzpicture}
\caption{Un ejemplo de convolución bidimensional \cite{goodfellow2016deep}}
\label{convolucion}
\end{figure}

Cuando el kernel es mas chico que una imagen, como se observa en la
figura \ref{convolucion}, las conexiones entre los datos de entrada
y los de salida son escasas, al contrario que en las tradicionales
redes en las cuales una neurona de una capa oculta intermedia está
completamente conectada con todas las neuronas de la capa anterior.

% Hasta acá, un resumen con otras palabras del libro de Goodfellow
\clearpage


\section{Estado del arte}

\subsection{Variational autoencoders}
Los VAE (\textit{Variational AutoEncoders}, por sus siglas en inglés)
son una variante de los autoencoders (AE) tradicionales, del cual
modifican la forma en que la arquitectura representa los objetos en
el espacio de códigos latentes.  

En los AE, el \textit{encoder} representa los objetos en un punto en
el espacio, mientras que en los VAE estos se representan como una
distribución \cite{Foster2019}. La ventaja de esto es que brinda
continuidad, al no ser ya un punto discontinuo de otros objetos sino
teniendo en cuenta que el espacio representa la probabilidad de ser un
objeto u otro, permitiendo transiciones más suaves en el espacio de
códigos latentes de un objeto a otro.

Al terminar el entrenamiento de un \textit{autoencoder} no se obtienen
buenos resultados al generar una muestra aleatoria en el espacio de
los códigos latentes
% Revisar aca si ponemos el caso de MNIST que probé, sería lo más sensato
% En el paper de SDF hacen eso, ponen MNIST para mostrar por que usan un auto-decoder.

\subsection{Arquitecturas de Deep Learning en 3D}

En \cite{Gao2019} las redes aprenden un conjunto de voxeles deformables
para poder reconstruir figuras suaves, en lugar de reconstruir usando
voxeles. Su arquitectura consta de dos VAE: La primera, que llaman
``PartVAE'' se entrena un dataset con figuras etiquetadas por partes,
por lo que aprende a reconstruir las partes por separado. La segunda,
que llaman ``SP-VAE'' (por ``Structured Parts VAE'') aprende la
estructura global del objeto, asegurando luego la coherencia de las
partes ensambladas. En \cite{Muralikrishnan2019}, se utiliza la
arquitectura VAE y la representación de objetos 3D en forma de voxeles,
puntos y multiples vistas 2D para construir un mismo código latente
que representa al mismo objeto, independientemente de su
representación original y que permite reconstruir el mismo objeto en
cualquiera de las tres formas de representación. Esta arquitectura se
aplica tanto para tareas discriminativas como generativas.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%       
%%%%% Apartado nuevo debatiendo las ideas de SDF de Park2019 %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%       
\cite{Park2019} propone una red que representa los objetos tridimensionales
en un campo volumétrico cuyo valor en cualquier punto representa la 
distancia de ese punto a la superficie del objeto, y su signo representa si
el punto se encuenta en el interior o en el exterior del objeto. De esta
forma, la superficie se encuentra en el valor cero devuelto por la función
aprendida por la red, llamada \textit{SDF} (\textit{SignedDistance
Function} por sus siglas en inglés).

Los datos usados para entrenar el modelo consisten en un muestreo de puntos
cercanos a la superficie de objetos representados como mallas de triangulos.
Luego, a cada punto se le aplica la función SDF, obteniendo un valor para
cada punto, siendo la combinación de los puntos y su valor de SDF los datos
de entrenamiento.

La arquitectura entrenada consiste en un \textit{autodecoder}: solo el
código latente y el decoder de un tradicional autoencoder es usado. Para
cada uno de los elementos del dataset de entrenamiento, se inicializa un
código latente aleatorio que luego es modificado por el algoritmo de
\textit{backpropagation}. La forma de operar es similar a un
\textit{autoencoder}, el codigo latente representa una codificación de SDF,
que luego es utilizado para reconstruir cada objeto usando la función SDF
inferida por la red. Como resultados comparables con otro tipo de redes,
la arquitectura propuesta logra captar detalles finos de los objetos y
puede reconstruirlos con una alta resolucion. No obstante, en objetos
muy detallados puede fallar en reconstruirlos. Otra de los logros de la
red es poder disminuir el tamaño en memoria de los objetos y del modelo
debido a la elección de representación de los mismos elegida. Esto hace
posible un entrenamiento más rápido y menores necesidades computacionales
que otras representaciones para una misma representación. No obstante,
\cite{Park2019} reporta que la forma habitual de entrenamiento de un
VAE no les ha dado resultado para este tipo de problemas, por lo que
podría suponerse que esta red tiene problemas para generar nuevos objetos % Esta parte es suposición, no sé si conviene dejarla.
creando nuevos códigos latentes para alimentar el \textit{autodecoder}.
La forma de entrenamiento tampoco permite conocer la distribución a
posteriori del espacio de códigos latentes del dataset de entrenamiento.
Otro de los problemas al intentar usar esta red es debido a que la red
entrenada es específica de los datos usados, ya que no hay
\textit{encoder} que permita generar nuevos codigos latentes a partir
de nuevos datos. Esto es, la red aprende solo una representación
comprimida de los datos de entrenamiento.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Apartado viejo Park2019 %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% También existen otras formas de representaciones con performances
% destacables, como eluso de funciones de distancia con signo \cite{Park2019}
% (Signed Distance Functions, SDF, por sus siglas en inglés) en el que
% se muestrean puntos cerca de la superficie de un objeto teniendo en
% cuenta si el punto se encuentra dentro o fuera de la figura
% (representado por el signo) y la distancia más corta del punto hacia
% la superficie del objeto a entrenar. Esto permite que se calcule una
% SDF por cada una de las figuras de entrenamiento, teniendo luego un
% auto encoder (o auto decoder, un tipo de arquitectura que no usa
% encoders para entrenar, como en el caso de \cite{Park2019}) que
% realizar una inferencia de las SDF de cada figura y encontrar
% abstracciones entre las distintas SDF de cada figura de los datos de
% entrenamiento.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Apartado nuevo debatiendo las ideas de TBN de Olszewski2019 %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
TBN \cite{Olszewski2019} es una red que infiere objetos tridimensionales
a partir de vistas bidimensionales y genera nuevas vistas del mismo,
realizando transformaciones sobre el espacio tridimensional aprendidas
por la red al incluir las transformaciones junto a los datos de
entrenamiento.

Los datos de entrenamiento consisten en imágenes a transformar en
tridimensionales así como también las transformaciones a aplicar a
los objetos. De esta manera, se espera que la red aprenda tanto a
transformar un objeto bidimensional a tridimensional así como también a
aplicar las transformaciones sobre los mismos para generar nuevas vistas
bidimensionales (NVS, o \textit{Novel View Synthesis}, por sus siglas en
inglés).

La arquitectura consiste en un \textit{autoencoder} modificado, el cual
conserva las partes \textit{encoder-decoder} e incluye un
\textit{bottleneck}, o cuello de botella, entre ambos, en donde ocurren
las manipulaciones espaciales de los objetos tridimensionales. Estas
transformaciones pueden ser variadas, pudiendo interactuar con un objeto
de diversas formas. Otra de las diferencias con un \textit{autoencoder}
tradicional reside en que el \textit{encoder} realiza la transformación
de un espacio bidimensional a uno tridimensional. Tanto el \textit{encoder}
como el \textit{decoder} son redes convolucionales pero contienen un paso
intermedio de \textit{reshaping} para el cambio de dimensiones. Por ende,
el \textit{bottleneck} contiene un objeto tridimensional, no un código
latente. Además, el usuario provee la función de transformación para este
objeto en el \textit{bottleneck}. 
La funcion de pérdida utilizada para entrenar la red tiene varias
componentes:
\begin{itemize}
    \item Pérdida de reconstucción \(L_{1}\).
    \item Pérdida perceptual, definida como la pérdida en el espacio de
        características de la red VGG-19 \cite{Simonyan2015}
    \item Pérdida de similitud %TODO: Revisar las citas que tiene respecto
                               % de este tipo de pérdida, para poder anotar
                               % la matemática de la misma.
    \item Pérdida adversarial, utilizando el discriminador de una red GAN.
    \item Pérdida de segmentación: para cada una de las imágenes, tanto el
        \textit{input} como el \textit{output} tienen una máscara binaria
        (ceros y unos) que identifican los píxeles donde se encuentra el
        objeto con 1 y el resto con 0. De esta forma, se puede medir el
        resultado de la segmentación lograda por el decoder.
\end{itemize}

\begin{figure}[h]
\includegraphics[height=4cm]{images/Olszewski2019_arq.png}
\centering
\caption{Arquitectura de TBN \cite{Wu2016} para NVS usando un \textit{bottleneck} tridimensional}
\label{TBN_arq}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Apartado viejo debatiendo las ideas de TBN de Olszewski2019 %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Hay nuevas arquitecturas \cite{Olszewski2019} que
% también han logrado generar vistas nuevas de objetos tridimensionales
% (Novel View Synthesis, o NVS por sus siglas en inglés). En el caso de
% \cite{Olszewski2019}, se entrena usando varios objetos con múltiples
% vistas (imágenes en dos dimensiones) de un mismo objeto tridimensional
% en una arquitectura con encoder, un "bottleneck" (o "cuello de botella")
% y un decoder para recrear la imagen con las transformaciones aplicadas
% en el bottleneck. Lo interesante del método empleado consiste en que la
% NVS no es algo inferido por la red en el entrenamiento, sino que es
% aplicado de una forma no aprendida por la red en el bottleneck, lo que
% permite realizar otras transformaciones además de nuevas vistas de un
% objeto (como, por ejemplo, distintas poses de un ser humano) sin variar
% la forma de entrenar la red. Asimismo, las interpolaciones son
% diferenciables y permiten el correcto funcionamiento del algoritmo de
% backpropagation para poder actualizar los parámetros del encoder.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Apartado nuevo Li2019 %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textit{Part Aware Generative Network}, PAGENet \cite{Li2019}, es una
arquitectura que consiste en varias redes que actuan sobre las partes
de una serie de objetos tridimensionales voxelizados y segmentados
semánticamente en sus partes, con otra red encargada del ensamblado
de las partes para obtener la reconstrucción de los datos de entrada.
Con este tipo de arquitectura se pretende solucionar un problema
recurrente en la reconstrucción de figuras en zonas de alta varianza
en los datos, correspondiente en general a detalles más finos, sin
incrementar la resolución.
La arquitectura de PAGENet consiste en varias partes. En primer lugar
están los generadores por parte, que consisten en redes VAE-GAN, cuya
misión es reconstruir las partes semánticas del objeto.
Para la reconstrucción de las partes se usa una función de pérdida que
incluye las siguientes componentes:
* Pérdida de reconstrucción.
* Divergencia Kullback-Leibler.
* Pérdida de reflexión: para las partes simétricas, se computa esta
pérdida si la reconstrucción no lo es.
* Pérdida adversarial: Correspondiente al discriminador de la red GAN.
El VAE actúa como generador.

Usando los objetos reconstruidos de estas redes se encuentra otra red
encargada del ensamblado de las partes generadas previamente,
trabajando directamente en el ensamblado de los objetos.
Como las redes individuales en ningun momento tratan el tamaño
adecuado de las partes para lograr la correcta
reconstrucción del objeto, de ello se tiene que encargar la
arquitectura de ensamblado, encargándose de todas las
transformaciones necesarias a cada una de las partes. Esta
red tiene cinco capas convolucionales con tamaños de
kernel de \(4x4x4\) junto con las habituales capas de normalización
por lotes y capas ReLU. Por lo tanto, el output no es directamente
un objeto, sino una serie de transformaciones a aplicar a
las partes recibidas de las redes generadoras para finalmente obtener
el objeto reconstruido. Asimismo, para dar variedad a los objetos
producidos, se puede dejar constante en 1 el valor de las
transformaciones de una de las partes, llamada \textit{ancla}, para
si transformar las otras y que se amolden al ancla. Esto hace
que en cada momento haya tantas posibles reconstrucciones de un mismo
objeto como partes haya, ya que cualquiera de ellas puede elegirse
como ancla.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Apartado viejo Li2019 %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \cite{Li2019} logra una mejor reconstrucción de figuras en tres dimensiones
% si se reconstruyen las partes por separado (teniendo datos de entrenamiento
% con partes claramente segmentadas, con el fin de poder dividirlas y usar
% distintos modelos para aprenderlas) y luego se ensamblan de acuerdo a la
% forma en que se corresponden las distintas partes, pudiendo tener leves
% variaciones respecto a las piezas originales pero correspondiéndose las
% partes entre sí a la hora del ensamble. Asimismo, eligiendo una de las
% piezas como ancla para el ensamble, pueden lograr generar distintas
% figuras dentro del espacio de figuras estudiado. La arquitectura que usan
% corresponde a un VAE-GAN: un variational autoencoder cuya reconstrucción
% de salida es la entrada de una red GAN. Esto genera un código latente
% formado por los códigos de las partes individuales, lo que alimenta una
% última red que es la encargada del ensamble de las partes de la figura.

\begin{figure}[h]
\includegraphics[height=4cm]{images/Li2019_arq.png}
\centering
\caption{Arquitectura de PAGENet \cite{Li2019} para la generación de objetos tridimensionales ensamblando sus partes}
\label{PAGENet_arq}
\end{figure}

\cite{Yin2019} propone una arquitectura llamada LOGAN, por sus siglas en inglés de
\textit{Latent Overcomplete Generative Adversarial Network} con el propósito de
lograr traducciones de figuras entre dos dominios distintos pero con características
similares, como pueden ser sillas y mesas, de una forma no supervisada, es decir,
no hay ninguna segmentación previa, ninguna correspondencia entre puntos realizada
entre figuras de los dos dominios ni ningún tipo de emparejamiento de los dos
dominios previo al entrenamiento, sino a ser encontrado por la red luego de
terminado el entrenamiento.
La arquitectura está compuesta por dos partes, un autoencoder y dos traductores que
van en direcciones opuestas: uno traduce del dominio \textit{x} al \textit{y} y el
otro, en el sentido opuesto. Estos traductores tienen forma de red GAN. Tanto los
autoencoders como los traductores son entrenados de forma independiente entre sí
pero siguiendo un orden: primero se entrena el autoencoder y luego, los traductores.
La particularidad en el entrenamiento del autoencoder consiste en que los vectores
latentes se forman a partir de las capas PointNet++ \cite{Qi2017} del encoder, de
forma tal que forman un vector con la concatenación de las partes (tipo
\textit{overcomplete}) y otros vectores con las partes individuales rellenos de
ceros para completar el tamaño de vector necesario para alimentar el decoder. A su
vez, se alimenta el decoder con todos estos vectores, de forma tal que la red puede
aprender distintas características de forma independiente, lo que lleva a un
\textit{disentangling} implícito en las partes que conforman el vector
\textit{overcomplete}. Como función de pérdida se usa EMD (Earth Mover's Distance,
por sus siglas en inglés). Luego se entrenan por separado las redes traductoras,
cuya función de pérdida es algo más compleja para permitir la correcta traducción
entre los dominios. La primera componente es la función de pérdida adversarial de
la WGAN \cite{pmlr-v70-arjovsky17a} a la que se suma una componente de preservación
de características, que mide el error de traducción entre una figura de un dominio
\textit{x} a su mismo dominio, teniendo que replicar la misma figura. Esto permite
que la red aprenda a preservar las caracteristicas importantes de la figura y a que
la traducción al dominio \textit{y} preserve los aspectos más dominantes de la
figura, haciendo sólo algunos cambios en la traducción. Por último, lo que detallan
como una componente menor sólo para prevenir que un grupo de figuras del dominio
\textit{y} abarquen todas las traducciones desde el dominio \textit{x}, agregan una
última componente llamada \textit{de ciclo}, la cual comprende la pérdida de volver
a traducir una figura desde \textit{y} hacia \textit{x} que previamente había sido
traducida en el sentido contrario, teniendo que volverse a reconstruir la figura
original, forzando a que haya más relaciones unívocas entre dominios.  

\begin{figure}[h]
\includegraphics[height=4cm]{images/Yin2019_arq_autoencoder.png}
\centering
\caption{Arquitectura de autoencoder en LOGAN \cite{Yin2019} para la generación de objetos tridimensionales a partir de códigos latentes \textit{overcomplete}}
\label{LOGAN_arq_ae}
\end{figure}

\bibliographystyle{plain}
\addcontentsline{toc}{section}{\refname}
\bibliography{tesis_dm}

\end{document}
