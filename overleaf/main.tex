\documentclass[spanish]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{indentfirst}
\usepackage[a4paper, margin=1 in]{geometry}
\usepackage[spanish]{babel}

% Paquetes para usar graficos desde mathcha.io
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{tikz}
\usepackage{mathdots}
\usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\graphicspath{ {./images/} }
\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        

\begin{document}

\input{plan_tesis_title_page.tex}
\input{abstract_es.tex}
\input{abstract_en.tex}

\section{Introducción}

Dentro de la ciencia de datos actual, una de las áreas que más se
están desarrollando en los últimos tiempos es la relacionada a las
arquitecturas de redes neuronales y, en particular, a aquellas relacionadas
con redes profundas, o ``Deep Learning'' en inglés.

Teóricamente, cualquier red neuronal de tres capas (entrada, oculta
y de salida) puede aproximar cualquier función, pero aún se desconoce
el entrenamiento apropiado para poder alcanzar esa meta teórica.
Por ende, se recurrió a agregar capas no redundantes que permiten
alcanzar mejores resultados a los que se estaban logrando a través
de las redes neuronales de tres capas, dando lugar a las llamadas
redes profundas.

Las redes profundas incidieron de manera decisiva en el procesamiento
de imágenes en dos y tres dimensiones. Se han desarrollado y se continúan
desarrollando redes con cada vez mejor precisión a la hora de reconocer
y reconstruir figuras de las más diversas. Para la reconstrucción
de modelos tridimensionales, las arquitecturas más comunes son los ``autoencoders'',
las llamadas ``Generative Adversarial Networks'' (GAN) y los ``variational
autoencoders'' (VAE). Recientemente se han comenzaron a desarrollar
arquitecturas que, además de poder reconstruir los ejemplos de entrenamiento,
puede generar nuevas imágenes, tomando nuevas muestras de datos de
un espacio continuo multivariado \cite{Karras2018}.

Un campo que está comenzando a tomar impulso recientemente es el de
procesamiento de datos en tres dimensiones usando redes generativas
con arquitecturas como GAN y VAE. Estas redes permiten, además de
reconstruir figuras con las que se entrenen, ajustar una distribución
multivariada (ajustada a tener una distribución normal estándar multivariada)
a los fines de tener un espacio de muestreo continuo a través del cual poder generar
nuevas figuras y poder hacer una interpolación continua en el espacio continuo de
las figuras de entrenamiento. Esto permite poder explorar el espacio completo de las
figuras de entrenamiento, con la posibilidad de encontrar figuras relacionadas a las
originales pero con variaciones en los detalles, los cuales podrían ser útiles a los
fines de entender y explorar el espacio de figuras con las que se trabajó en los
datos originales.

\subsection{Convoluciones, pooling, normalización por lotes}

% En la versión de 2017 del libro "Deep Learning" de Goodfellow, Bengio y Courville
% a partir de la página 330 comienza el capítulo de redes convolucionales.
Las redes convolucionales son redes neuronales en las que al menos una de sus capas
usan una operación de convolución en lugar de una multiplicación matricial
\cite{goodfellow2016deep}. Este tipo de capas es fundamental para el procesamiento
de tipos de datos que presentan un formato de cuadrícula, como las imágenes
bidimensionales o los objetos tridimensionales. Es por ello que son uno de los
bloques fundamentales para las redes que tratan con este tipo de datos.

La convolución es una operación matemática que consiste en una media ponderada sobre
los datos de entrada (en el caso de \(t\) discreto):

\[s(t) = (x \circledast w)(t) = \sum_{a = -\infty}^{\infty} x(a)w(t-a) \]

Donde \(x\) representa los datos de entrada, \(w\) son los pesos del promedio
ponderado, que también se denominan \textit{kernel}.
Para el caso discreto bidimensional discreto (imágenes, por ejemplo), en lugar de
una suma infinita sólo se realiza sobre la suma de los valores de las matrices de
entrada a la convolución. También el kernel \(w\) toma la forma de una matriz
bidimensional con pesos adaptables por la red. Por ende, en el caso de una imagen
\(I\) con un kernel \(K\):

\[S(i,j)=(I \circledast K)(i,j)=\sum_{m}^{n} I(m,n)K(i-m,j-n)\]

Por ejemplo, al aplicar la convolución a una matriz de entrada bidimensional:

\begin{figure}[H]
\centering
\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,460); %set diagram left start at 0, and has height of 460

\draw   (115,33) -- (165,33) -- (165,83) -- (115,83) -- cycle ; %Shape: Square [id:dp008348541907688967] 
\draw   (177,33) -- (227,33) -- (227,83) -- (177,83) -- cycle ; %Shape: Square [id:dp5695495374345101] 
\draw   (238,33) -- (288,33) -- (288,83) -- (238,83) -- cycle ; %Shape: Square [id:dp08913217477595681] 
\draw   (300,33) -- (350,33) -- (350,83) -- (300,83) -- cycle ; %Shape: Square [id:dp09565678498451557] 
\draw   (115,93) -- (165,93) -- (165,143) -- (115,143) -- cycle ; %Shape: Square [id:dp6616061503281512] 
\draw   (177,93) -- (227,93) -- (227,143) -- (177,143) -- cycle ; %Shape: Square [id:dp9731322863310219] 
\draw   (238,93) -- (288,93) -- (288,143) -- (238,143) -- cycle ; %Shape: Square [id:dp1734488588540104] 
\draw   (300,93) -- (350,93) -- (350,143) -- (300,143) -- cycle ; %Shape: Square [id:dp783784734771654] 
\draw   (115,153) -- (165,153) -- (165,203) -- (115,203) -- cycle ; %Shape: Square [id:dp3810445646111744] 
\draw   (177,153) -- (227,153) -- (227,203) -- (177,203) -- cycle ; %Shape: Square [id:dp597477459287147] 
\draw   (238,153) -- (288,153) -- (288,203) -- (238,203) -- cycle ; %Shape: Square [id:dp3259061174876716] 
\draw   (300,153) -- (350,153) -- (350,203) -- (300,203) -- cycle ; %Shape: Square [id:dp9728278094881708] 
\draw   (112,29) -- (230,29) -- (230,147) -- (112,147) -- cycle ; %Shape: Square [id:dp8155679517421655] 
\draw   (399,60) -- (449,60) -- (449,110) -- (399,110) -- cycle ; %Shape: Square [id:dp394644813826424] 
\draw   (461,60) -- (511,60) -- (511,110) -- (461,110) -- cycle ; %Shape: Square [id:dp8614370121086297] 
\draw   (399,120) -- (449,120) -- (449,170) -- (399,170) -- cycle ; %Shape: Square [id:dp595415369573254] 
\draw   (461,120) -- (511,120) -- (511,170) -- (461,170) -- cycle ; %Shape: Square [id:dp8309992031835938] 
\draw   (396,56) -- (514,56) -- (514,174) -- (396,174) -- cycle ; %Shape: Square [id:dp77375541391222] 
\draw   (117,233) -- (213.5,233) -- (213.5,329.5) -- (117,329.5) -- cycle ; %Shape: Square [id:dp2906572996014607] 
\draw   (111,228) -- (219.5,228) -- (219.5,336.5) -- (111,336.5) -- cycle ; %Shape: Square [id:dp4766592365283635] 
\draw   (224,234) -- (320.5,234) -- (320.5,330.5) -- (224,330.5) -- cycle ; %Shape: Square [id:dp690329106444943] 
\draw   (330,234) -- (426.5,234) -- (426.5,330.5) -- (330,330.5) -- cycle ; %Shape: Square [id:dp14628491601258453] 
\draw   (117,344) -- (213.5,344) -- (213.5,440.5) -- (117,440.5) -- cycle ; %Shape: Square [id:dp268759623182556] 
\draw   (224,345) -- (320.5,345) -- (320.5,441.5) -- (224,441.5) -- cycle ; %Shape: Square [id:dp5482508582899983] 
\draw   (330,345) -- (426.5,345) -- (426.5,441.5) -- (330,441.5) -- cycle ; %Shape: Right Angle [id:dp2438723945511756] 
\draw   (62.09,279.98) -- (62.09,86.11) -- (111.33,86.11) ; %Straight Lines [id:da4032615230182004] 
\draw   (62.09,279.98) -- (109.33,279.98) ;
\draw [shift={(111.33,279.98)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0}][line width=0.75] (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29);


\draw (140,58) node   [align=left] {$\displaystyle a$}; % Text Node
\draw (202,58) node   [align=left] {$\displaystyle b$}; % Text Node
\draw (264,58) node   [align=left] {$\displaystyle c$}; % Text Node
\draw (326,58) node   [align=left] {$\displaystyle d$}; % Text Node
\draw (140,118) node   [align=left] {$\displaystyle e$}; % Text Node
\draw (202,118) node   [align=left] {$\displaystyle f$}; % Text Node
\draw (264,118) node   [align=left] {$\displaystyle g$}; % Text Node
\draw (326,118) node   [align=left] {$\displaystyle h$}; % Text Node
\draw (140,178) node   [align=left] {$\displaystyle i$}; % Text Node
\draw (202,178) node   [align=left] {$\displaystyle j$}; % Text Node
\draw (264,178) node   [align=left] {$\displaystyle k$}; % Text Node
\draw (326,178) node   [align=left] {$\displaystyle l$}; % Text Node
\draw (129,16) node   [align=left] {Input}; % Text Node
\draw (424,85) node   [align=left] {$\displaystyle w$}; % Text Node
\draw (486,85) node   [align=left] {$\displaystyle x$}; % Text Node
\draw (424,145) node   [align=left] {$\displaystyle y$}; % Text Node
\draw (486,145) node   [align=left] {$\displaystyle z$}; % Text Node
\draw (418,43) node   [align=left] {Kernel}; % Text Node
% Text Node
\draw (169,284) node   [align=left] {$\displaystyle
\begin{array}{{>{\displaystyle}l}}
aw\ +bx\ +\ \\
ey\ \ +\ fz
\end{array}$};
% Text Node
\draw (276,285) node   [align=left] {$\displaystyle
\begin{array}{{>{\displaystyle}l}}
bw\ +cx\ +\ \\
fy\ \ +\ gz
\end{array}$};
% Text Node
\draw (382,286) node   [align=left] {$\displaystyle
\begin{array}{{>{\displaystyle}l}}
cw\ +dx\ +\ \\
gy\ \ +\ hz
\end{array}$};
% Text Node
\draw (168,393) node   [align=left] {$\displaystyle
\begin{array}{{>{\displaystyle}l}}
ew\ +fx\ +\ \\
iy\ \ +\ jz
\end{array}$};
% Text Node
\draw (275,394) node   [align=left] {$\displaystyle
\begin{array}{{>{\displaystyle}l}}
fw\ +gx\ +\ \\
jy\ \ +\ kz
\end{array}$};
% Text Node
\draw (381,395) node   [align=left] {$\displaystyle
\begin{array}{{>{\displaystyle}l}}
gw\ +hx\ +\ \\
ky\ \ +\ lz
\end{array}$};

\end{tikzpicture}
\caption{Un ejemplo de convolución bidimensional \cite{goodfellow2016deep}}
\label{convolucion}
\end{figure}

Cuando el kernel es mas chico que una imagen, como se observa en la
figura \ref{convolucion}, las conexiones entre los datos de entrada
y los de salida son escasas, al contrario que en las tradicionales
redes en las cuales una neurona de una capa oculta intermedia está
completamente conectada con todas las neuronas de la capa anterior.

% Hasta acá, un resumen con otras palabras del libro de Goodfellow
\clearpage


\section{Estado del arte}

\subsection{Variational autoencoders}
Los VAE (\textit{Variational AutoEncoders}, por sus siglas en inglés)
son una variante de los autoencoders (AE) tradicionales, del cual
modifican la forma en que la arquitectura representa los objetos en
el espacio de códigos latentes.  

En los AE, el \textit{encoder} representa los objetos en un punto en
el espacio, mientras que en los VAE estos se representan como una
distribución \cite{Foster2019}. La ventaja de esto es que brinda
continuidad, al no ser ya un punto discontinuo de otros objetos sino
teniendo en cuenta que el espacio representa la probabilidad de ser un
objeto u otro, permitiendo transiciones más suaves en el espacio de
códigos latentes de un objeto a otro.

Al terminar el entrenamiento de un \textit{autoencoder} no se obtienen
buenos resultados al generar una muestra aleatoria en el espacio de
los códigos latentes
% Revisar aca si ponemos el caso de MNIST que probé, sería lo más sensato
% En el paper de SDF hacen eso, ponen MNIST para mostrar por que usan un auto-decoder.


\subsection{Arquitecturas de Deep Learning en 3D}

En \cite{Gao2019} las redes aprenden un conjunto de voxeles deformables
para poder reconstruir figuras suaves, en lugar de reconstruir usando
voxeles. Su arquitectura consta de dos VAE: La primera, que llaman
``PartVAE'' se entrena un dataset con figuras etiquetadas por partes,
por lo que aprende a reconstruir las partes por separado. La segunda,
que llaman ``SP-VAE'' (por ``Structured Parts VAE'') aprende la
estructura global del objeto, asegurando luego la coherencia de las
partes ensambladas. En \cite{Muralikrishnan2019}, se utiliza la
arquitectura VAE y la representación de objetos 3D en forma de voxeles,
puntos y multiples vistas 2D para construir un mismo código latente
que representa al mismo objeto, independientemente de su
representación original y que permite reconstruir el mismo objeto en
cualquiera de las tres formas de representación. Esta arquitectura se
aplica tanto para tareas discriminativas como generativas.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%       
%%%%% Apartado nuevo debatiendo las ideas de SDF de Park2019 %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%       
\cite{Park2019} propone una red que representa los objetos tridimensionales
en un campo volumétrico cuyo valor en cualquier punto representa la 
distancia de ese punto a la superficie del objeto, y su signo representa si
el punto se encuenta en el interior o en el exterior del objeto. De esta
forma, la superficie se encuentra en el valor cero devuelto por la función
aprendida por la red, llamada \textit{SDF} (\textit{SignedDistance
Function} por sus siglas en inglés).

Los datos usados para entrenar el modelo consisten en un muestreo de puntos
cercanos a la superficie de objetos representados como mallas de triangulos.
Luego, a cada punto se le aplica la función SDF, obteniendo un valor para
cada punto, siendo la combinación de los puntos y su valor de SDF los datos
de entrenamiento.

La arquitectura entrenada consiste en un \textit{autodecoder}: solo el
código latente y el decoder de un tradicional autoencoder es usado. Para
cada uno de los elementos del dataset de entrenamiento, se inicializa un
código latente aleatorio que luego es modificado por el algoritmo de
\textit{backpropagation}. La forma de operar es similar a un
\textit{autoencoder}, el codigo latente representa una codificación de SDF,
que luego es utilizado para reconstruir cada objeto usando la función SDF
inferida por la red. Como resultados comparables con otro tipo de redes,
la arquitectura propuesta logra captar detalles finos de los objetos y
puede reconstruirlos con una alta resolucion. No obstante, en objetos
muy detallados puede fallar en reconstruirlos. Otra de los logros de la
red es poder disminuir el tamaño en memoria de los objetos y del modelo
debido a la elección de representación de los mismos elegida. Esto hace
posible un entrenamiento más rápido y menores necesidades computacionales
que otras representaciones para una misma representación. No obstante,
\cite{Park2019} reporta que la forma habitual de entrenamiento de un
VAE no les ha dado resultado para este tipo de problemas, por lo que
podría suponerse que esta red tiene problemas para generar nuevos objetos % Esta parte es suposición, no sé si convenga ponerla.
creando nuevos códigos latentes para alimentar el \textit{autodecoder}.
La forma de entrenamiento tampoco permite conocer la distribución a
posteriori del espacio de códigos latentes del dataset de entrenamiento.
Otro de los problemas al intentar usar esta red es debido a que la red
entrenada es específica de los datos usados, ya que no hay
\textit{encoder} que permita generar nuevos codigos latentes a partir
de nuevos datos. Esto es, la red aprende solo una representación
comprimida de los datos de entrenamiento.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%       
%%%%% Apartado viejo debatiendo las ideas de SDF de Park2019 %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%       
% También existen otras formas de representaciones con performances
% destacables, como eluso de funciones de distancia con signo \cite{Park2019}
% (Signed Distance Functions, SDF, por sus siglas en inglés) en el que
% se muestrean puntos cerca de la superficie de un objeto teniendo en
% cuenta si el punto se encuentra dentro o fuera de la figura
% (representado por el signo) y la distancia más corta del punto hacia
% la superficie del objeto a entrenar. Esto permite que se calcule una
% SDF por cada una de las figuras de entrenamiento, teniendo luego un
% auto encoder (o auto decoder, un tipo de arquitectura que no usa
% encoders para entrenar, como en el caso de \cite{Park2019}) que
% realizar una inferencia de las SDF de cada figura y encontrar
% abstracciones entre las distintas SDF de cada figura de los datos de
% entrenamiento.

Hay nuevas arquitecturas \cite{Olszewski2019} que
también han logrado generar vistas nuevas de objetos tridimensionales
(Novel View Synthesis, o NVS por sus siglas en inglés). En el caso de
\cite{Olszewski2019}, se entrena usando varios objetos con múltiples
vistas (imágenes en dos dimensiones) de un mismo objeto tridimensional
en una arquitectura con encoder, un "bottleneck" (o "cuello de botella")
y un decoder para recrear la imagen con las transformaciones aplicadas
en el bottleneck. Lo interesante del método empleado consiste en que la
NVS no es algo inferido por la red en el entrenamiento, sino que es
aplicado de una forma no aprendida por la red en el bottleneck, lo que
permite realizar otras transformaciones además de nuevas vistas de un
objeto (como, por ejemplo, distintas poses de un ser humano) sin variar
la forma de entrenar la red. Asimismo, las interpolaciones son
diferenciables y permiten el correcto funcionamiento del algoritmo de
backpropagation para poder actualizar los parámetros del encoder.

% Agregar figura de la arquitectura de la TBN de Olszewski et al.

\cite{Li2019} logra una mejor reconstrucción de figuras en tres dimensiones
si se reconstruyen las partes por separado (teniendo datos de entrenamiento
con partes claramente segmentadas, con el fin de poder dividirlas y usar
distintos modelos para aprenderlas) y luego se ensamblan de acuerdo a la
forma en que se corresponden las distintas partes, pudiendo tener leves
variaciones respecto a las piezas originales pero correspondiéndose las
partes entre sí a la hora del ensamble. Asimismo, eligiendo una de las
piezas como ancla para el ensamble, pueden lograr generar distintas
figuras dentro del espacio de figuras estudiado. La arquitectura que usan
corresponde a un VAE-GAN: un variational autoencoder cuya reconstrucción
de salida es la entrada de una red GAN. Esto genera un código latente
formado por los códigos de las partes individuales, lo que alimenta una
última red que es la encargada del ensamble de las partes de la figura.

% Agregar diagrama de la arquitectura usada por Li2019

\cite{Yin2019} propone una arquitectura llamada LOGAN, por sus siglas en inglés de \textit{Latent Overcomplete Generative Adversarial Network} con el propósito de lograr traducciones de figuras entre dos dominios distintos pero con características similares, como pueden ser sillas y mesas, de una forma no supervisada, es decir, no hay ninguna segmentación previa, ninguna correspondencia entre puntos realizada entre figuras de los dos dominios ni ningún tipo de emparejamiento de los dos dominios previo al entrenamiento, sino a ser encontrado por la red luego de terminado el entrenamiento.
La arquitectura está compuesta por dos partes, un autoencoder y dos traductores que van en direcciones opuestas: uno traduce del dominio \textit{x} al \textit{y} y el otro, en el sentido opuesto. Estos traductores tienen forma de red GAN. Tanto los autoencoders como los traductores son entrenados de forma independiente entre sí pero siguiendo un orden: primero se entrena el autoencoder y luego, los traductores. La particularidad en el entrenamiento del autoencoder consiste en que los vectores latentes se forman a partir de las capas PointNet++ \cite{Qi2017} del encoder, de forma tal que forman un vector con la concatenación de las partes (tipo \textit{overcomplete}) y otros vectores con las partes individuales rellenos de ceros para completar el tamaño de vector necesario para alimentar el decoder. A su vez, se alimenta el decoder con todos estos vectores, de forma tal que la red puede aprender distintas características de forma independiente, lo que lleva a un \textit{disentangling} implícito en las partes que conforman el vector \textit{overcomplete}. Como función de pérdida se usa EMD (Earth Mover's Distance, por sus siglas en inglés). Luego se entrenan por separado las redes traductoras, cuya función de pérdida es algo más compleja para permitir la correcta traducción entre los dominios. La primera componente es la función de pérdida adversarial de la WGAN \cite{pmlr-v70-arjovsky17a} a la que se suma una componente de preservación de características, que mide el error de traducción entre una figura de un dominio \textit{x} a su mismo dominio, teniendo que replicar la misma figura. Esto permite que la red aprenda a preservar las caracteristicas importantes de la figura y a que la traducción al dominio \textit{y} preserve los aspectos más dominantes de la figura, haciendo sólo algunos cambios en la traducción. Por último, lo que detallan como una componente menor sólo para prevenir que un grupo de figuras del dominio \textit{y} abarquen todas las traducciones desde el dominio \textit{x}, agregan una última componente llamada \textit{de ciclo}, la cual comprende la pérdida de volver a traducir una figura desde \textit{y} hacia \textit{x} que previamente había sido traducida en el sentido contrario, teniendo que volverse a reconstruir la figura original, forzando a que haya más relaciones unívocas entre dominios.  

\bibliographystyle{plain}
\addcontentsline{toc}{section}{\refname}
\bibliography{tesis_dm}

\end{document}
